import streamlit as st
from dotenv import load_dotenv
import os
from typing import List
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS

# LangChain ìµœì‹  1.0 API
from langchain_openai import ChatOpenAI
from langchain_core.messages import (
    HumanMessage,
    AIMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.tools import tool
from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolSelectorMiddleware
import pytz
from langchain_classic.tools.retriever import create_retriever_tool
from langchain_core.vectorstores.base import VectorStore
import traceback
import inspect
import time
import base64
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter


# .envì—ì„œ OPENAI_API_KEY ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
# api_key = os.getenv("OPENAI_API_KEY")


# # -------------------------------
# # 1ï¸âƒ£ ë„êµ¬ ì •ì˜
# # -------------------------------
# @tool
# def get_current_time(timezone: str, location: str) -> str:
#     """í˜„ì¬ ì‹œê°„ì„ ì§€ì •ëœ íƒ€ì„ì¡´ê³¼ ìœ„ì¹˜ì— ë§ê²Œ ë°˜í™˜í•©ë‹ˆë‹¤."""
#     import pytz
#     from datetime import datetime
#     try:
#         tz = pytz.timezone(timezone)
#         now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
#         return f'{timezone} ({location}) í˜„ì¬ì‹œê° {now}'
#     except pytz.UnknownTimeZoneError:
#         return f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì„ì¡´: {timezone}"

# @tool
# def get_web_search(query: str, search_period: str) -> str:
#     """DuckDuckGo APIë¥¼ ì´ìš©í•´ ì§€ì •ëœ ê¸°ê°„ ë‚´ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
#     wrapper = DuckDuckGoSearchAPIWrapper(region="kr-kr", time=search_period)
#     search = DuckDuckGoSearchResults(api_wrapper=wrapper, source="news", results_separator=';\n')
#     return search.invoke(query)

# -------------------------------
# 2ï¸âƒ£ LLM ë° ì—ì´ì „íŠ¸ ìƒì„±
# -------------------------------

#tools = [get_current_time, get_web_search]
#tool_dict = {tool.name: tool for tool in tools}
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.4, api_key=api_key)
agent = create_agent(
    model=llm,
    # tools=[get_current_time, get_web_search],
    # middleware=[LLMToolSelectorMiddleware(max_tools=2)]
    )


# def get_ai_response(messages):
#     try:
#         response = agent.stream({"messages":messages})
#         response = {"message": messages}
#         gathered = None
#         for chunk in agent.stream(response, stream_mode="updates"):
#             yield chunk
#             if gathered is None:
#                 gathered = chunk
#             else:
#                 gathered += chunk

#         if gathered and getattr(gathered, "tool_calls", None):
#             st.session_state.messages.append(gathered)
#             for tool_call in gathered.tool_calls:
#                 selected_tool = tool_dict.get(tool_call['name'])
#                 if selected_tool:
#                     with st.spinner("ë„êµ¬ ì‹¤í–‰ ì¤‘..."):
#                         try:
#                             tool_msg = selected_tool.invoke(tool_call)
#                             st.session_state.messages.append(tool_msg)
#                         except Exception as e:
#                             st.error(f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜:{e}")
#             # ë„êµ¬ í˜¸ì¶œ í›„ ì¬ê·€ì ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
#             yield from get_ai_response(st.session_state["messages"])


#          # AIì˜ ìµœì¢… ì‘ë‹µì´ ìˆìœ¼ë©´ ì´ë¥¼ ì¶œë ¥
#         if gathered:
#             # gatheredê°€ ìµœì¢…ì ìœ¼ë¡œ AI ì‘ë‹µì„ í¬í•¨í•˜ê³  ìˆìœ¼ë©´ ì´ë¥¼ ì¶œë ¥
#             ai_response = gathered.get('content', '')  # AI ì‘ë‹µì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ê¸°

#             if ai_response:
#                 # Streamlitì— AIì˜ ì‘ë‹µ ì¶œë ¥
#                 st.write(f"AI ì‘ë‹µ: {ai_response}")
#                 # í•„ìš”ì‹œ UIë¥¼ í†µí•´ 'ëŒ€í™”' í˜•ì‹ìœ¼ë¡œ ì‘ë‹µì„ ì¶”ê°€
#                 st.session_state.messages.append({"role": "ai", "content": ai_response})


#     except Exception as e:
#         st.error(f"âŒ invoke() í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# def answer_question(query: str, timeout_sec: int = 60):
#     """LLM ê¸°ë°˜ PDF QA """

#     st.write("ğŸš€ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘")
#     start_time = time.time()

#     vectorstore = st.session_state.get("vectorstore")
#     if vectorstore is None:
#         st.warning("âš ï¸ PDF í•™ìŠµì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
#         return "ë¨¼ì € PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  í•™ìŠµì‹œì¼œ ì£¼ì„¸ìš”."

#     st.write("âœ… vectorstore í™•ì¸ ì™„ë£Œ")

#     try:
#         # ë¬¸ì„œì—ì„œ ìœ ì‚¬ë„ ê²€ì‚¬
#         docs_with_scores = vectorstore.similarity_search_with_score(query, k=3)
        
#         st.write(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰ íšŸìˆ˜: {len(docs_with_scores)}íšŒ")
        
#         # ë””ë²„ê¹…: ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ
#         for i, (doc, score) in enumerate(docs_with_scores, 1):
#             st.write(f"  ë¬¸ì„œ {i} ìœ ì‚¬ë„: {score:.4f}")
        
#         # ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •
#         SIMILARITY_THRESHOLD = 0.9
        
#         relevant_docs = [doc for doc, score in docs_with_scores if score < SIMILARITY_THRESHOLD]
        
#         if not relevant_docs:
#             st.warning(f"âš ï¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìµœì†Œ ìœ ì‚¬ë„: {min(score for _, score in docs_with_scores):.4f})")
#             return "ì£„ì†¡í•©ë‹ˆë‹¤. "
        
#         st.success(f"âœ… {len(relevant_docs)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

#         # Retriever ìƒì„± 
#         retriever = vectorstore.as_retriever(
#             search_type="similarity", 
#             search_kwargs={"k": 3}
#         )
#         st.write("âœ… retriever ìƒì„± ì™„ë£Œ")

       
#         # QA Chain ìƒì„±
#         qa_chain = create_retriever_tool(
#             retriever=retriever,
#             name="document_search",
#             description="ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
#             )
#         st.write("âœ… ìœ ì‚¬ë„ ì—°ê²° ìƒì„± ì™„ë£Œ")

#         # ì§ˆë¬¸ ì‹¤í–‰
#         try:
#             with st.spinner("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘..."):
#                 result = qa_chain.invoke({"query": query})
#         except Exception as e:
#             st.error(f"âŒ invoke() í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#             st.code(traceback.format_exc(), language="python")
#             return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        
#         elapsed = time.time() - start_time
#         st.success(f"âœ… ì‘ë‹µ ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")

#         # ê²°ê³¼ ì¶”ì¶œ
#         if isinstance(result, dict):
#             answer = result.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
#             # LLMì´ "ê´€ë ¨ ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µí•œ ê²½ìš° ê°ì§€
#             if "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in answer or "ê´€ë ¨ì´ ì—†" in answer:
#                 st.info("ğŸ’¡ í•™ìŠµëœ ë¬¸ì„œì™€ ì§ˆë¬¸ì´ ê´€ë ¨ì´ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
            
#             # ì¶œì²˜ ë¬¸ì„œ í‘œì‹œ (ì„ íƒì‚¬í•­)
#             if result.get("source_documents"):
#                 with st.expander("ğŸ“š ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
#                     for i, doc in enumerate(result["source_documents"], 1):
#                         st.text_area(f"ë¬¸ì„œ {i}", doc.page_content[:300], height=200)
            
#             return answer
#         else:
#             return str(result)

#     except Exception as e:
#         st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         st.code(traceback.format_exc(), language="python")
#         return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    

# def process1_f(uploaded_files1):
#     """PDF íŒŒì¼ì„ í•™ìŠµí•˜ì—¬ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    
#     # íŒŒì¼ ê°œìˆ˜ ì²´í¬
#     if uploaded_files1 and len(uploaded_files1) > 3:
#         st.error("âŒ PDFëŠ” ìµœëŒ€ 3ê°œê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
#         st.warning("âš ï¸ PDFíŒŒì¼ì„ 3ê°œë§Œ ì„ íƒí•˜ì—¬ ì£¼ì„¸ìš”!")
#         return None  # ì—¬ê¸°ì„œ ë°”ë¡œ return
    
#     # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
#     if not uploaded_files1:
#         st.warning("âš ï¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
#         return None

#     try:
#         with st.spinner("ğŸ“š PDF ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
#             all_splits = []
            
#             # ê° PDF íŒŒì¼ ì²˜ë¦¬
#             for idx, uploaded_file in enumerate(uploaded_files1, 1):
#                 st.write(f"ğŸ“„ {idx}/{len(uploaded_files1)} íŒŒì¼ ì²˜ë¦¬ ì¤‘: {uploaded_file.name}")
                
#                 # ì„ì‹œ íŒŒì¼ ìƒì„±
#                 with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
#                     tmp_file.write(uploaded_file.read())
#                     tmp_path = tmp_file.name

#                 try:
#                     # PDF ë¡œë“œ
#                     loader = PyPDFLoader(tmp_path)
#                     data = loader.load()
                    
#                     # ì²­í‚¹
#                     splitter = RecursiveCharacterTextSplitter(
#                         chunk_size=300, 
#                         chunk_overlap=50
#                     )
#                     splits = splitter.split_documents(data)
#                     all_splits.extend(splits)
                    
#                     st.success(f"âœ… {uploaded_file.name}: {len(splits)}ê°œ ë¬¸ì„œë¡œ ë¶„í• ")
                    
#                 finally:
#                     # ì„ì‹œ íŒŒì¼ ì‚­ì œ
#                     if os.path.exists(tmp_path):
#                         os.remove(tmp_path)

#             # ì´ ì²­í¬ ìˆ˜ í‘œì‹œ
#             st.info(f"ğŸ“Š ì´ ë¬¸ì„œ ë¶„í•  ìˆ˜: {len(all_splits)}")

#             # Embedding ìƒì„±
#             embedding = OpenAIEmbeddings(
#                 model="text-embedding-3-large", 
#                 api_key=st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
#             )
            
#             # ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
#             persist_directory = "c:/faiss_store"
#             os.makedirs(persist_directory, exist_ok=True)

#             # ë°°ì¹˜ ë‹¨ìœ„ ì„ë² ë”©
#             batch_size = 20
#             vectorstore = None
#             total_batches = (len(all_splits) + batch_size - 1) // batch_size
            
#             progress_bar = st.progress(0)
#             status_text = st.empty()
            
#             for i in range(0, len(all_splits), batch_size):
#                 batch = all_splits[i:i+batch_size]
#                 batch_num = i//batch_size + 1
                
#                 status_text.text(f"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} í•™ìŠµìë£Œ ì €ì¥ ì¤‘...")
#                 progress_bar.progress(batch_num / total_batches)
                
#                 try:
#                     if vectorstore is None:
#                         # ì²« ë°°ì¹˜ë¡œ vectorstore ìƒì„±
#                         vectorstore = FAISS.from_documents(batch, embedding)
#                     else:
#                         # ê¸°ì¡´ vectorstoreì— ì¶”ê°€
#                         vectorstore.add_documents(batch)
                    
#                     # ë¡œì»¬ì— ì €ì¥
#                     vectorstore.save_local(persist_directory)
#                     time.sleep(1.5)  # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
                    
#                 except Exception as e:
#                     st.error(f"âŒ ë°°ì¹˜ {batch_num} í•™ìŠµìë£Œ ì €ì¥ ì‹¤íŒ¨: {e}")
#                     continue

#             progress_bar.progress(1.0)
#             status_text.text("âœ… í•™ìŠµìë£Œ ì €ì¥ ì™„ë£Œ!")
            
#             st.success("ğŸ‰ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
#             # st.balloons()
#             st.toast("í•™ìŠµí•œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”!", icon="ğŸ‰")
#             return vectorstore
            
#     except Exception as e:
#         st.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         st.code(traceback.format_exc(), language="python")
#         return None    



# --- Streamlit ì•± ì„¤ì • ---
st.set_page_config(page_title="AI Chat", page_icon="ğŸ’¬", layout="wide")

st.title("ğŸ’¬ ê³ ì„±êµ°ì²­ AI Chatbot ë„ìš°ë¯¸")

# --- í™”ë©´ ë””ìì¸ ---
st.markdown("""
    <style>
    /* ê¸°ë³¸ ë°”ë”” í°íŠ¸ ë° ë°°ê²½ */
    body {
        background-color: #f0f2f6;
        font-family: 'Noto Sans KR', sans-serif;
        color: #333;
    }

    /* ì‚¬ì´ë“œë°” ë°°ê²½ê³¼ ê·¸ë¦¼ì */
    [data-testid="stSidebar"] {
        background: #ffffff;
        border-right: none;
        box-shadow: 2px 0 8px rgba(0, 0, 0, 0.1);
        padding: 1rem 1.5rem;
    }

    /* ì‚¬ì´ë“œë°” ê° ì„¹ì…˜ ë°•ìŠ¤ ìŠ¤íƒ€ì¼ */
    .sidebar-section {
        background: #fafafa;
        border-radius: 12px;
        padding: 20px 25px;
        margin-bottom: 25px;
        box-shadow: 0 3px 8px rgba(0,0,0,0.05);
        transition: box-shadow 0.3s ease;
    }

    .sidebar-section:hover {
        box-shadow: 0 5px 15px rgba(0,0,0,0.1);
    }

    /* ì„¹ì…˜ ì œëª© ìŠ¤íƒ€ì¼ */
    .sidebar-section h2, .sidebar-section h3 {
        font-weight: 700;
        color: #1f2937;  /* ì–´ë‘ìš´ ë„¤ì´ë¹„ í†¤ */
        margin-bottom: 12px;
        display: flex;
        align-items: center;
        gap: 8px;
    }

    /* ì•„ì´ì½˜ í¬ê¸° ì¡°ì ˆ */
    .sidebar-section h2 svg, .sidebar-section h3 svg {
        width: 24px;
        height: 24px;
        fill: #3b82f6; /* íŒŒë€ìƒ‰ í†¤ */
    }

    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ ê°œì„  */
    .stButton>button {
        background-color: #3b82f6;
        color: white;
        border-radius: 10px;
        padding: 10px 18px;
        font-weight: 600;
        font-size: 16px;
        border: none;
        transition: background-color 0.25s ease;
        width: 100%;
        cursor: pointer;
    }

    </style>
""", unsafe_allow_html=True)




with st.sidebar:
    st.header("âš™ï¸ ë¬¸ì„œ :red[í•™ìŠµê¸°]")
    uploaded_files1 = st.file_uploader(
    "ğŸ“ í•™ìŠµ ë¬¸ì„œ ì—…ë¡œë“œ ì°½ :red[PDF]íŒŒì¼  :red[3]ê°œë§Œ ê°€ëŠ¥", type=['pdf'], accept_multiple_files=True
    )
    process1 = st.button("ğŸš€ í•™ìŠµì‹œì‘",        
            type = "primary",
            disabled=(uploaded_files1 is None))

    st.markdown("---")
    st.markdown("### ğŸ“– :blue[ì‚¬ìš©ë°©ë²•]")
    st.markdown("""
        1. PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”(ìµœëŒ€ 3ê°œë§Œ)
        2. "í•™ìŠµì‹œì‘"  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
        3. í•™ìŠµí•œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼
        ë‹µë³€í•©ë‹ˆë‹¤. 
        """)
        
    st.markdown("---")

    
       

# ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage(content="ì €ëŠ” ê³ ì„±êµ°ì²­ ì§ì›ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "),  
        AIMessage(content="ë¬´ì—‡ì„ ë„ì™€ ë“œë¦´ê¹Œìš”?")
    ]

# í•™ìŠµ dataê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
# if "vectorstore" not in st.session_state:
#     st.session_state["vectorstore"] = None

# ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state["messages"]:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)
        # elif isinstance(msg, ToolMessage):
        #     st.chat_message("tool").write(msg.content)


# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input(placeholder = "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”?"):
    st.chat_message("user").write(prompt) # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    st.session_state["messages"].append(HumanMessage(prompt)) # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥

response = agent.stream(prompt)
st.session_state["message"].append(AIMessage(response))
st.chat_message("assistant").write_stream(response)

# user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
# if user_input:
#     st.session_state.messages.append(HumanMessage(content=user_input))
#     with st.chat_message("user"):
#         st.markdown(user_input)

#     with st.chat_message("assistant"):
#         message_placeholder = st.empty()
#         try:
#             # LangChain 1.0 ë°©ì‹ìœ¼ë¡œ invoke ì‹¤í–‰
#             response = agent.invoke({
#                     "messages": [HumanMessage(content=user_input)]
#                      })
#             ai_reply = response.get("output", "(ì‘ë‹µ ì—†ìŒ)")
#             st.session_state.messages.append(AIMessage(content=ai_reply))
#             message_placeholder.markdown(ai_reply)

#         except Exception as e:
#             st.error("âŒ ì˜¤ë¥˜ ë°œìƒ:")
#             st.code(traceback.format_exc(), language="python")


        # vectorstore ì¡´ì¬ ì—¬ë¶€ í™•ì¸
#    vectorstore = st.session_state.get("vectorstore")
    
#    if vectorstore is not None:
#       # ë²¡í„°ìŠ¤í† ì–´ ê¸°ë°˜ ë‹µë³€
#        with st.spinner("ğŸ“š í•™ìŠµëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
#            answer = answer_question(prompt)
        
        # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜
    #     if answer and "ì£„ì†¡í•©ë‹ˆë‹¤. " in answer and len(answer) < 20:
    #         st.info("ğŸ’¡ í•™ìŠµëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ AI ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
    #         # st.write([type(m) for m in "messages"])
    #         response = get_ai_response(st.session_state["messages"])
    #         result = st.chat_message("assistant").markdown(response)
    #         st.write(1)
    #         st.session_state["messages"].append(AIMessage(content=str(result)))
    #     else:
    #         # ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
    #         st.write(answer)
    #         st.write(3)
    #         st.chat_message("assistant").write(answer)
    #         st.session_state.append(AIMessage(content=str(answer)))
    # else:
    #     # ì¼ë°˜ AI ëª¨ë“œ
#         st.info("ğŸ¤– ì¼ë°˜ AI ëª¨ë“œë¡œ ë‹µë³€í•©ë‹ˆë‹¤. ë¬¸ì„œë¥¼ í•™ìŠµí•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
#         st.write([type(m) for m in "messages"])
#         response = get_ai_response(st.session_state["messages"])
#         result = st.chat_message("assistant").write(response)
#         st.session_state["messages"].append(AIMessage(content=str(result)))


# # ë¬¸ì„œ í•™ìŠµ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
# if process1:
#     st.session_state["vectorstore"] = process1_f(uploaded_files1)




# '''
# # -------------------------------
# # 3ï¸âƒ£ Streamlit ì„¸ì…˜ ì´ˆê¸°í™”m
# # -------------------------------
# # if "messages" not in st.session_state:
# #     st.session_state["messages"] = [
# #         SystemMessage("ì €ëŠ” ê³ ì„±êµ°ì²­ ì§ì›ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "),  
# #         AIMessage("ë¬´ì—‡ì„ ë„ì™€ ë“œë¦´ê¹Œìš”?")
# #     ]

# # -------------------------------
# # 4ï¸âƒ£ ë©”ì‹œì§€ UI í‘œì‹œ
# # -------------------------------
# # for msg in st.session_state.messages:
# #     role = "user" if isinstance(msg, HumanMessage) else "assistant"
# #     with st.chat_message(role):
# #         st.markdown(msg.content)

# # -------------------------------
# # 5ï¸âƒ£ ì‚¬ìš©ì ì…ë ¥ ë° ì‘ë‹µ ì²˜ë¦¬
# # -------------------------------
# user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
# if user_input:
#     st.session_state.messages.append(HumanMessage(content=user_input))
#     with st.chat_message("user"):
#         st.markdown(user_input)

#     with st.chat_message("assistant"):
#         message_placeholder = st.empty()
#         try:
#             # LangChain 1.0 ë°©ì‹ìœ¼ë¡œ invoke ì‹¤í–‰
#             response = agent.invoke({
#                     "messages": [HumanMessage(content=user_input)]
#                      })
#             ai_reply = response.get("output", "(ì‘ë‹µ ì—†ìŒ)")
#             st.session_state.messages.append(AIMessage(content=ai_reply))
#             message_placeholder.markdown(ai_reply)

#         except Exception as e:
#             st.error("âŒ ì˜¤ë¥˜ ë°œìƒ:")
#             st.code(traceback.format_exc(), language="python")




# #             for chunk in agent.stream({
# #     "messages": [{"role": "user", "content": "Search for AI news and summarize the findings"}]
# # }, stream_mode="values"):
# #     # Each chunk contains the full state at that point
# #     latest_message = chunk["messages"][-1]
# #     if latest_message.content:
# #         print(f"Agent: {latest_message.content}")
# #     elif latest_message.tool_calls:
# #         print(f"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}")
# '''