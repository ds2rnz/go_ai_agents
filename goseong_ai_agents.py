import streamlit as st
from langchain_community.chat_models import ChatOpenAI
from langchain.tools import tool
from langchain.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from datetime import datetime
from langchain.agents import create_agent

import pytz
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from dotenv import load_dotenv
import os

from typing import TypedDict, Annotated, List
from langgraph.graph import StateGraph, END
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import ast
import operator

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from openai import OpenAI
from langchain_classic.tools.retriever import create_retriever_tool

import concurrent.futures
import traceback
import inspect
import time
import base64
import tempfile

# ë„êµ¬ ì •ì˜
@tool
def get_current_time(timezone: str, location: str) -> str:
    """í˜„ì¬ ì‹œê°„ì„ ì§€ì •ëœ íƒ€ì„ì¡´ê³¼ ìœ„ì¹˜ì— ë§ê²Œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    import pytz
    from datetime import datetime
    try:
        tz = pytz.timezone(timezone)
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        return f'{timezone} ({location}) í˜„ì¬ì‹œê° {now}'
    except pytz.UnknownTimeZoneError:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì„ì¡´: {timezone}"

@tool
def get_web_search(query: str, search_period: str) -> str:
    """DuckDuckGo APIë¥¼ ì´ìš©í•´ ì§€ì •ëœ ê¸°ê°„ ë‚´ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    wrapper = DuckDuckGoSearchAPIWrapper(region="kr-kr", time=search_period)
    search = DuckDuckGoSearchResults(api_wrapper=wrapper, source="news", results_separator=';\n')
    return search.invoke(query)


load_dotenv()

OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))


# OPENAI_API_KEY = "OPENAI_API_KEY"


client = OpenAI(api_key = "OPENAI_API_KEY")   

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.4, # ì •í™•ë„  0.0 ~ 1.0
    timeout=30,  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
    max_retries=2 ) 

tools = [get_current_time, get_web_search]

agent = create_agent(
    model=llm,
    tools=tools
)



tools = [get_current_time, get_web_search]
tool_dict = {tool.name: tool for tool in tools}
llm_with_tools = agent # tool ì‚¬ìš© llm ì •ì˜


# @debug_wrap / ì—ëŸ¬ í™•ì¸ í•¨ìˆ˜ ìš”ì²­
def get_ai_response(messages, thread_id: str = "default"):
    config =  {"configurable": {"thread_id": thread_id}}
    gathered = None
    for chunk in llm_with_tools.stream(
        {"messages": [{"role": "user", "content": messages}]},
        config,
        stream_mode="values"
    ):
        yield chunk["messages"][-1].content
        if gathered is None:
            gathered = chunk
        else:
            gathered += chunk

    if gathered and getattr(gathered, "tool_calls", None):
        st.session_state["messages"].append(gathered)
        for tool_call in gathered.tool_calls:
            selected_tool = tool_dict.get(tool_call['name'])
            if selected_tool:
                with st.spinner("ë„êµ¬ ì‹¤í–‰ ì¤‘..."):
                    tool_msg = selected_tool.invoke(tool_call)
                    st.session_state["messages"].append(tool_msg)
        # ë„êµ¬ í˜¸ì¶œ í›„ ì¬ê·€ì ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        yield from get_ai_response(st.session_state["messages"])


# @debug_wrap / ì—ëŸ¬ í™•ì¸ í•¨ìˆ˜ ìš”ì²­
def answer_question(query: str, timeout_sec: int = 60):
    """LLM ê¸°ë°˜ PDF QA """

    st.write("ğŸš€ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘")
    start_time = time.time()

    vectorstore = st.session_state.get("vectorstore")
    if vectorstore is None:
        st.warning("âš ï¸ PDF í•™ìŠµì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return "ë¨¼ì € PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  í•™ìŠµì‹œì¼œ ì£¼ì„¸ìš”."

    st.write("âœ… vectorstore í™•ì¸ ì™„ë£Œ")

    try:
        # ë¬¸ì„œì—ì„œ ìœ ì‚¬ë„ ê²€ì‚¬
        docs_with_scores = vectorstore.similarity_search_with_score(query, k=3)
        
        st.write(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰ íšŸìˆ˜: {len(docs_with_scores)}íšŒ")
        
        # ë””ë²„ê¹…: ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ
        for i, (doc, score) in enumerate(docs_with_scores, 1):
            st.write(f"  ë¬¸ì„œ {i} ìœ ì‚¬ë„: {score:.4f}")
        
        # ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •
        SIMILARITY_THRESHOLD = 1.1
        
        relevant_docs = [doc for doc, score in docs_with_scores if score < SIMILARITY_THRESHOLD]
        
        if not relevant_docs:
            st.warning(f"âš ï¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìµœì†Œ ìœ ì‚¬ë„: {min(score for _, score in docs_with_scores):.4f})")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. "
        
        st.success(f"âœ… {len(relevant_docs)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

        # Retriever ìƒì„± 
        retriever = vectorstore.as_retriever(
            search_type="similarity", 
            search_kwargs={"k": 3}
        )
        st.write("âœ… retriever ìƒì„± ì™„ë£Œ")

       
        # QA Chain ìƒì„±
        qa_chain = create_retriever_tool(
            llm=llm,  # llm ê°€ì ¸ì˜¤ê¸°
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            )
        st.write("âœ… ìœ ì‚¬ë„ ì—°ê²° ìƒì„± ì™„ë£Œ")

        # ì§ˆë¬¸ ì‹¤í–‰
        try:
            with st.spinner("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘..."):
                result = qa_chain.invoke({"query": query})
        except Exception as e:
            st.error(f"âŒ invoke() í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            st.code(traceback.format_exc(), language="python")
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        
        elapsed = time.time() - start_time
        st.success(f"âœ… ì‘ë‹µ ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")

        # ê²°ê³¼ ì¶”ì¶œ
        if isinstance(result, dict):
            answer = result.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # LLMì´ "ê´€ë ¨ ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µí•œ ê²½ìš° ê°ì§€
            if "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in answer or "ê´€ë ¨ì´ ì—†" in answer:
                st.info("ğŸ’¡ í•™ìŠµëœ ë¬¸ì„œì™€ ì§ˆë¬¸ì´ ê´€ë ¨ì´ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
            
            # ì¶œì²˜ ë¬¸ì„œ í‘œì‹œ (ì„ íƒì‚¬í•­)
            if result.get("source_documents"):
                with st.expander("ğŸ“š ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
                    for i, doc in enumerate(result["source_documents"], 1):
                        st.text_area(f"ë¬¸ì„œ {i}", doc.page_content[:300], height=200)
            
            return answer
        else:
            return str(result)

    except Exception as e:
        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.code(traceback.format_exc(), language="python")
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    


# @debug_wrap / ì—ëŸ¬ í™•ì¸ í•¨ìˆ˜ ìš”ì²­
def process1_f(uploaded_files1):
    """PDF íŒŒì¼ì„ í•™ìŠµí•˜ì—¬ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    
    # íŒŒì¼ ê°œìˆ˜ ì²´í¬
    if uploaded_files1 and len(uploaded_files1) > 3:
        st.error("âŒ PDFëŠ” ìµœëŒ€ 3ê°œê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
        st.warning("âš ï¸ PDFíŒŒì¼ì„ 3ê°œë§Œ ì„ íƒí•˜ì—¬ ì£¼ì„¸ìš”!")
        return None  # ì—¬ê¸°ì„œ ë°”ë¡œ return
    
    # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
    if not uploaded_files1:
        st.warning("âš ï¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return None

    try:
        with st.spinner("ğŸ“š PDF ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
            all_splits = []
            
            # ê° PDF íŒŒì¼ ì²˜ë¦¬
            for idx, uploaded_file in enumerate(uploaded_files1, 1):
                st.write(f"ğŸ“„ {idx}/{len(uploaded_files1)} íŒŒì¼ ì²˜ë¦¬ ì¤‘: {uploaded_file.name}")
                
                # ì„ì‹œ íŒŒì¼ ìƒì„±
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(uploaded_file.read())
                    tmp_path = tmp_file.name

                try:
                    # PDF ë¡œë“œ
                    loader = PyPDFLoader(tmp_path)
                    data = loader.load()
                    
                    # ì²­í‚¹
                    splitter = RecursiveCharacterTextSplitter(
                        chunk_size=300, 
                        chunk_overlap=50
                    )
                    splits = splitter.split_documents(data)
                    all_splits.extend(splits)
                    
                    st.success(f"âœ… {uploaded_file.name}: {len(splits)}ê°œ ë¬¸ì„œë¡œ ë¶„í• ")
                    
                finally:
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                    if os.path.exists(tmp_path):
                        os.remove(tmp_path)

            # ì´ ì²­í¬ ìˆ˜ í‘œì‹œ
            st.info(f"ğŸ“Š ì´ ë¬¸ì„œ ë¶„í•  ìˆ˜: {len(all_splits)}")

            # Embedding ìƒì„±
            embedding = OpenAIEmbeddings(
                model="text-embedding-3-large", 
                api_key="OPENAI_API_KEY"
            )
            
            # ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            persist_directory = "c:/faiss_store"
            os.makedirs(persist_directory, exist_ok=True)

            # ë°°ì¹˜ ë‹¨ìœ„ ì„ë² ë”©
            batch_size = 20
            vectorstore = None
            total_batches = (len(all_splits) + batch_size - 1) // batch_size
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i in range(0, len(all_splits), batch_size):
                batch = all_splits[i:i+batch_size]
                batch_num = i//batch_size + 1
                
                status_text.text(f"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} í•™ìŠµìë£Œ ì €ì¥ ì¤‘...")
                progress_bar.progress(batch_num / total_batches)
                
                try:
                    if vectorstore is None:
                        # ì²« ë°°ì¹˜ë¡œ vectorstore ìƒì„±
                        vectorstore = FAISS.from_documents(batch, embedding)
                    else:
                        # ê¸°ì¡´ vectorstoreì— ì¶”ê°€
                        vectorstore.add_documents(batch)
                    
                    # ë¡œì»¬ì— ì €ì¥
                    vectorstore.save_local(persist_directory)
                    time.sleep(1.5)  # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
                    
                except Exception as e:
                    st.error(f"âŒ ë°°ì¹˜ {batch_num} í•™ìŠµìë£Œ ì €ì¥ ì‹¤íŒ¨: {e}")
                    continue

            progress_bar.progress(1.0)
            status_text.text("âœ… í•™ìŠµìë£Œ ì €ì¥ ì™„ë£Œ!")
            
            st.success("ğŸ‰ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            # st.balloons()
            st.toast("í•™ìŠµí•œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”!", icon="ğŸ‰")
            return vectorstore
            
    except Exception as e:
        st.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.code(traceback.format_exc(), language="python")
        return None




# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê³ ì„±êµ°ì²­ AI ë„ìš°ë¯¸", 
    page_icon="ğŸ¤–", 
    layout="wide",
    initial_sidebar_state="expanded"  
)

st.markdown("""
    <style>
    /* ê¸°ë³¸ ë°”ë”” í°íŠ¸ ë° ë°°ê²½ */
    body {
        background-color: #f0f2f6;
        font-family: 'Noto Sans KR', sans-serif;
        color: #333;
    }

    
     /* ì…ë ¥ì°½ ìŠ¤íƒ€ì¼ */
    .stChatInput input {
        width: 60%;  /* ì…ë ¥ì°½ì˜ ë„ˆë¹„ë¥¼ 60%ë¡œ ì„¤ì • */
        height: 10%;  /* ë†’ì´ ì¦ê°€ */    
        border: 2px solid #3b82f6;
        border-radius: 25px;
        padding: 15px 25px;
        font-size: 16px;
        background: linear-gradient(to right, #f0f9ff, #ffffff);
        transition: all 0.3s ease;
    }
    
    .stChatInput input:focus {
        border-color: #2563eb;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1);
        background: white;
    }
    
    .stChatInput button {
        background: linear-gradient(135deg, #3b82f6, #2563eb);
        border-radius: 50%;
        transition: transform 0.3s ease;
    }
    
    .stChatInput button:hover {
        transform: scale(1.1) rotate(15deg);
    }

    </style>
""", unsafe_allow_html=True)

animated_input_css = """
    <style>
    /* ì…ë ¥ì°½ ë“±ì¥ ì• ë‹ˆë©”ì´ì…˜ */
    .stChatInput {
        animation: slide-up 0.5s ease-out;
    }
    
    @keyframes slide-up {
        from {
            opacity: 0;
            transform: translateY(50px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }
    
    /* íƒ€ì´í•‘ íš¨ê³¼ */
    .stChatInput input:focus {
        animation: typing-glow 2s ease-in-out infinite;
    }
    
    @keyframes typing-glow {
        0%, 100% { box-shadow: 0 0 5px rgba(59, 130, 246, 0.3); }
        50% { box-shadow: 0 0 20px rgba(59, 130, 246, 0.6); }
    }
    
    /* ë²„íŠ¼ íšŒì „ íš¨ê³¼ */
    .stChatInput button:hover {
        animation: rotate 0.5s ease;
    }
    
    @keyframes rotate {
        from { transform: rotate(0deg); }
        to { transform: rotate(360deg); }
    }
    </style>
"""

st.markdown(animated_input_css, unsafe_allow_html=True)

# íƒ€ì´í‹€
st.markdown("""
    <style>
        .centered-title {
            text-align: center;
            font-size: 3rem;
            color: #1e293b;
            margin-top: 0px;  /* ìœ„ìª½ ë§ˆì§„ */
            margin-bottom: 3px;  /* ì•„ë˜ìª½ ë§ˆì§„ */
            margin-left: 0px;  /* ì™¼ìª½ ë§ˆì§„ */
            margin-right: 0px;  /* ì˜¤ë¥¸ìª½ ë§ˆì§„ */
        }
        .ai-text {
            font-size: 3.5rem; /* AI ê¸€ì í¬ê¸° */
            color: #2563eb;
            margin-left: 10px; /* AI ë‹¨ì–´ ì™¼ìª½ì— ì—¬ë°± ì¶”ê°€ */
            margin-right: 10px; /* AI ë‹¨ì–´ ì˜¤ë¥¸ìª½ ì—¬ë°± ì¶”ê°€ */
        }
    </style> 
    <h1 style="text-align: center; font-size: 3rem; color: #1e293b;">
    ğŸ’¬ ê³ ì„±êµ°ì²­ <span class="ai-text">AI</span> ë„ìš°ë¯¸ </h>
                                
""", unsafe_allow_html=True)


# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.markdown('<div class="sidebar-box">', unsafe_allow_html=True)
    
# ë¬¸ì„œ í•™ìŠµê¸°
    st.markdown('<div class="sidebar-box">', unsafe_allow_html=True)
    
    st.markdown("""
        <h2 style="text-align: center; font-size: 1.7rem; color: #000000;">ğŸ“š ë¬¸ì„œ í•™ìŠµê¸°</h2>
        """, unsafe_allow_html=True)

    st.markdown("""
        <p class="upload-label">
            ğŸ“ PDF íŒŒì¼ ì—…ë¡œë“œ 
            <span class="badge">ìµœëŒ€ 3ê°œ</span>
        </p>
    """, unsafe_allow_html=True)
    
    uploaded_files1 = st.file_uploader(
        "í•™ìŠµí•  PDF ì„ íƒ",
        type=['pdf'],
        accept_multiple_files=True,
        key="uploader1",
        label_visibility="collapsed"
    )
    
    # ì—…ë¡œë“œëœ íŒŒì¼ í‘œì‹œ
    if uploaded_files1:
        st.markdown("""
            <div style="background: #f0fdf4; padding: 0.5rem; border-radius: 8px; margin-top: 0.5rem;">
                <p style="margin: 0; font-size: 0.85rem; color: #15803d; font-weight: 500;">
                    âœ… {}ê°œ íŒŒì¼ ì„ íƒë¨
                </p>
            </div>
        """.format(len(uploaded_files1)), unsafe_allow_html=True)
        
        for i, file in enumerate(uploaded_files1[:3], 1):
            st.markdown(f"""
                <div style="font-size: 0.8rem; color: #475569; padding: 0.2rem 0.5rem;">
                    {i}. {file.name[:30]}{'...' if len(file.name) > 30 else ''}
                </div>
            """, unsafe_allow_html=True)
    
    process1 = st.button(
        "ğŸš€ í•™ìŠµ ì‹œì‘",
        key="process1",
        type="primary",
        # disabled=(uploaded_files1 is None or len(uploaded_files1) == 0),
        use_container_width=True
    )
    
    # ì‚¬ìš©ë°©ë²•
    st.markdown("""
        <div class="usage-box">
            <div class="usage-title">
                ğŸ’¡ ì‚¬ìš©ë°©ë²•
            </div>
            <ol class="usage-list">
                <li>PDF íŒŒì¼ì„ ìµœëŒ€ 3ê°œê¹Œì§€ ì—…ë¡œë“œ</li>
                <li>"í•™ìŠµ ì‹œì‘" ë²„íŠ¼ í´ë¦­</li>
                <li>í•™ìŠµ ì™„ë£Œ í›„ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ê°€ëŠ¥</li>
            </ol>
        </div>
    """, unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
  
    
    # êµ¬ë¶„ì„ 
    st.markdown('<hr class="custom-divider">', unsafe_allow_html=True)
    
    # í•˜ë‹¨ ì •ë³´ 
    st.markdown("""
        <div style="text-align: center; padding: 1rem; color: #000000; font-size: 0.9rem;">
            <p style="margin: 0;">Made by ğŸ” ì´ë¬´í–‰ì •ê´€ ì •ë³´ê´€ë¦¬íŒ€</p>
            <p style="margin: 0.5rem 0 0 0;">v1.0.0 | 2025</p>
        </div>
    """, unsafe_allow_html=True)

   

# ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage("ì €ëŠ” ê³ ì„±êµ°ì²­ ì§ì›ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "),  
        AIMessage("ë¬´ì—‡ì„ ë„ì™€ ë“œë¦´ê¹Œìš”?")
    ]

if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None

# ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state["messages"]:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)
        # elif isinstance(msg, ToolMessage):
        #     st.chat_message("tool").write(msg.content)


# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input(placeholder="âœ¨ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”?"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
    st.chat_message("user").write(prompt)
    st.session_state["messages"].append(HumanMessage(prompt))

    # vectorstore ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    vectorstore = st.session_state.get("vectorstore")
    
    if vectorstore is not None:
        # ë²¡í„°ìŠ¤í† ì–´ ê¸°ë°˜ ë‹µë³€
        with st.spinner("ğŸ“š í•™ìŠµëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
            answer = answer_question(prompt)
        
        # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜
        if answer and "ì£„ì†¡í•©ë‹ˆë‹¤. " in answer and len(answer) < 20:
            st.info("ğŸ’¡ í•™ìŠµëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ AI ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            response = get_ai_response(st.session_state["messages"])
            result = st.chat_message("assistant").write_stream(response)
            st.session_state["messages"].append(AIMessage(result))
        else:
            # ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
            st.chat_message("assistant").write(answer)
            st.session_state["messages"].append(AIMessage(answer))
    else:
        # ì¼ë°˜ AI ëª¨ë“œ
        st.info("ğŸ¤– ì¼ë°˜ AI ëª¨ë“œë¡œ ë‹µë³€í•©ë‹ˆë‹¤. ë¬¸ì„œë¥¼ í•™ìŠµí•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        response = get_ai_response(st.session_state["messages"])
        result = st.chat_message("assistant").write_stream(response)
        st.session_state["messages"].append(AIMessage(result))


# ë¬¸ì„œ í•™ìŠµ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
if process1:
    st.session_state["vectorstore"] = process1_f(uploaded_files1)




