import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.tools import tool
from langchain.chat_models import init_chat_model
from langchain.agents import create_agent
from datetime import datetime
from langchain_community.tools.ddg_search import DuckDuckGoSearchRun
from langchain_community.tools.searx_search import SearxSearchResults
import pytz
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from dotenv import load_dotenv
import os
from langchain.messages import HumanMessage, ToolMessage, SystemMessage, AIMessage
from langgraph.checkpoint.memory import InMemorySaver
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_classic.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from pathlib import Path
import tempfile
import traceback
import time
import requests



@tool
def get_current_time(timezone: str, location: str) -> str:
    '''  í•´ë‹¹ ì§€ì—­ í˜„ì¬ì‹œê°ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ '''
    try:
        tz = pytz.timezone(timezone)
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        result = f'{timezone} ({location}) í˜„ì¬ì‹œê° {now}'
        return result
    except pytz.UnknownTimeZoneError:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì„ì¡´: {timezone}"  
    
def search_searx(messages) -> str:
    ''' searxë¥¼ í™œìš©í•œ ì¸í„°ë„· ê²€ìƒ‰ íˆ´'''
    # Searx ì¸ìŠ¤í„´ìŠ¤ URL
    searx_url = "https://searx.org/search"  # ê³µê°œëœ Searx ì¸ìŠ¤í„´ìŠ¤ URL
    params = {
        'q': messages,           # ê²€ìƒ‰í•  ì¿¼ë¦¬
        'format': 'json',     # ê²°ê³¼ í˜•ì‹ì„ JSONìœ¼ë¡œ ì§€ì •
        'engines': 'google,duckduckgo,bing', # êµ¬ì²´ì ìœ¼ë¡œ ê²€ìƒ‰í•  ì—”ì§„ì„ ì§€ì • (optional, ì—¬ëŸ¬ ì—”ì§„ì„ ì½¤ë§ˆë¡œ êµ¬ë¶„ ê°€ëŠ¥)
        'category': 'general' # ê²€ìƒ‰ ì¹´í…Œê³ ë¦¬ (optional)
    }
    
    try:
        response = requests.get(searx_url, params=params)
        st.write(response)
        response.raise_for_status()  # ì‘ë‹µì´ ì„±ê³µì ì´ì§€ ì•Šìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
        return response.json()  # JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ë°˜í™˜
    except requests.exceptions.RequestException as e:
        print(f"ê²€ìƒ‰ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None


def load_vectorstore(embedding, persist_directory="C:/faiss_store"):
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.isdir(persist_directory):
        # st.error(f"ğŸš¨ ì§€ì •ëœ ë””ë ‰í† ë¦¬ '{persist_directory}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
        # index.faiss íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    index_file = os.path.join(persist_directory, "index.faiss")
    pkl_file = os.path.join(persist_directory, "index.pkl")
    

    if os.path.exists(index_file) and os.path.exists(pkl_file):
        try:
            st.spinner("ğŸ“‚ ê¸°ì¡´ í•™ìŠµí•œ ìë£Œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
            vectorstore = FAISS.load_local(
                persist_directory, 
                embedding,
                allow_dangerous_deserialization=True  # í•„ìš”í•œ ê²½ìš°
            )
            # st.success("âœ… ê¸°ì¡´ í•™ìŠµìë£Œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤!")
            st.toast("ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤!", icon="ğŸ“š")
            return vectorstore
        except Exception as e:
            st.warning(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    else:
        return None        


def answer_question(query: str):
    st.write("ğŸš€ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘")
    vectorstore = st.session_state.get("vectorstore")
    if vectorstore is None:
        st.warning("âš ï¸ PDF í•™ìŠµì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return "ë¨¼ì € PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  í•™ìŠµì‹œì¼œ ì£¼ì„¸ìš”."

    st.write("âœ… vectorstore í™•ì¸ ì™„ë£Œ")
    try:
        docs_with_scores = vectorstore.similarity_search_with_score(query, k=3)
        for i, (doc, score) in enumerate(docs_with_scores, 1):
            st.write(f"  ë¬¸ì„œ {i} ìœ ì‚¬ë„: {score:.4f}")

        SIMILARITY_THRESHOLD = 1.1
        relevant_docs = [doc for doc, score in docs_with_scores if score < SIMILARITY_THRESHOLD]
        if not relevant_docs:
            st.warning("âš ï¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        


        template = """ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    
                    ë¬¸ì„œ ë‚´ìš©:
                    {context}

                    ì§ˆë¬¸: {question}

                    ë‹µë³€ ì‹œ ë‹¤ìŒì„ ì§€ì¼œì£¼ì„¸ìš”:
                    1. ë¬¸ì„œ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                    2. ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                    3. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

                    ë‹µë³€:"""

        prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )
        retriever = vectorstore.as_retriever(search_kwargs={"k":3})
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=False
        )
        result = qa_chain.invoke({"query": query})
        if isinstance(result, dict):
            return result.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            return str(result)
    except Exception as e:
        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.code(traceback.format_exc(), language="python")
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                

def ai_answer(messages):
    response = agent.invoke(
    {"messages": messages},
        config=config,
        tool_choice='any'  # ë„êµ¬ ì‚¬ìš© ê°•ì œ
    )
    return response


load_dotenv()
# OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ddg_search_tool = DuckDuckGoSearchRun()

search_searx = SearxSearchResults()

# LangChain Tool ê°ì²´ë¡œ ë˜í•‘
searx_tool = [
    Tool(
        name="Searx Search",
        func=search_searx.run,
        description="Searxë¥¼ í™œìš©í•œ ì›¹ê²€ìƒ‰."
    )
]

checkpointer = InMemorySaver()
config = {"configurable": {"thread_id": "1"}}

llm = init_chat_model(
    model = "openai:gpt-4o-mini",
    temperature=0.5, 
    max_tokens=1000, 
    timeout=10, 
    max_retries=2, 
    )

# Embedding ìƒì„±
embedding = OpenAIEmbeddings(
    model="text-embedding-3-large", 
    api_key=st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
    )

agent = create_agent(
    model=llm,
    tools=[get_current_time, ddg_search_tool, searx_tool],
    middleware=[],
    checkpointer=checkpointer,
    )





# --- Streamlit ì•± ì„¤ì • ---
st.set_page_config(page_title="GPT AI ë„ìš°ë¯¸", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ ê³ ì„±êµ°ì²­ AI ë„ìš°ë¯¸")

# --- í™”ë©´ ë””ìì¸ ---
st.markdown("""
    <style>
    /* CSS ìŠ¤íƒ€ì¼ì€ ê·¸ëŒ€ë¡œ */
    </style>
""", unsafe_allow_html=True)


with st.sidebar:
    st.header("âš™ï¸ ë¬¸ì„œ :red[í•™ìŠµê¸°]")
    uploaded_files1 = st.file_uploader(
    "ğŸ“ í•™ìŠµ ë¬¸ì„œ ì—…ë¡œë“œ ì°½ :red[PDF]íŒŒì¼  :red[3]ê°œë§Œ ê°€ëŠ¥", type=['pdf'], accept_multiple_files=True
    )
    process1 = st.button("ğŸš€ í•™ìŠµì‹œì‘",        
            type = "primary",
            disabled=(uploaded_files1 is None))

    st.markdown("---")
    st.markdown("### ğŸ“– :blue[ì‚¬ìš©ë°©ë²•]")
    st.markdown("""
        1. PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”(ìµœëŒ€ 3ê°œë§Œ)
        2. "í•™ìŠµì‹œì‘"  ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
        3. í•™ìŠµí•œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼
        ë‹µë³€í•©ë‹ˆë‹¤. 
        """)
        
    st.markdown("---")

    # í•˜ë‹¨ ì •ë³´ 
    st.markdown("""
        <div style="text-align: center; padding: 1rem; color: #000000; font-size: 0.9rem;">
            <p style="margin: 0;">Made by ğŸ” ì´ë¬´í–‰ì •ê´€ ì •ë³´ê´€ë¦¬íŒ€</p>
            <p style="margin: 0.5rem 0 0 0;">v1.0.0 | 2025</p>
        </div>
    """, unsafe_allow_html=True)




# ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system", "content": "ì €ëŠ” ê³ ì„±êµ°ì²­ ì§ì›ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
        {"role": "assistant", "content": "ë¬´ì—‡ì´ì„ ë„ì™€ ë“œë¦´ê¹Œìš”?"}
]

# ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    role = msg["role"]
    content = msg["content"]
    st.chat_message(role).write(content)

vectorstore = load_vectorstore(
    embedding=embedding,
    persist_directory="C:/faiss_store"
)

# í•™ìŠµ dataê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None


# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input(placeholder="ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”?"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° ì¶œë ¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    # vectorstore ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    vectorstore = st.session_state.get("vectorstore")

    if vectorstore is not None:
        # ë²¡í„°ìŠ¤í† ì–´ ê¸°ë°˜ ë‹µë³€
        with st.spinner("ğŸ“š í•™ìŠµëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
            answer = answer_question(prompt)

        # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜
        if answer and "ì£„ì†¡í•©ë‹ˆë‹¤. " in answer or len(answer) < 30:
            st.info("ğŸ’¡ í•™ìŠµëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ AI ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            # st.write([type(m) for m in "messages"])    
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                try:
                    response = ai_answer(st.session_state.messages)
                    ai_response = response['messages'][-1].content
                    st.toast("ì¼ë°˜ AI ëª¨ë“œë¡œ ë‹µë³€í•©ë‹ˆë‹¤....!", icon="ğŸ‰")
                    
                    # AI ë©”ì‹œì§€ ì¶”ê°€ ë° ì¶œë ¥
                    st.session_state.messages.append({"role": "assistant", "content": ai_response})
                    st.chat_message("assistant").write(ai_response)
                except Exception as e:
                    error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                    st.chat_message("assistant").write(error_msg)
        else:
            # ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
            st.session_state.messages.append({"role": "assistant", "content": answer})
            st.chat_message("assistant").write(answer)
    else:
        # ì¼ë°˜ AI ëª¨ë“œ
        # st.info("ğŸ¤– ì¼ë°˜ AI ëª¨ë“œë¡œ ë‹µë³€í•©ë‹ˆë‹¤. ë¬¸ì„œë¥¼ í•™ìŠµí•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                response = ai_answer(st.session_state.messages)
                ai_response = response['messages'][-1].content
                st.toast("ì¼ë°˜ AI ëª¨ë“œë¡œ ë‹µë³€í•©ë‹ˆë‹¤....!", icon="ğŸ‰")
                
                # AI ë©”ì‹œì§€ ì¶”ê°€ ë° ì¶œë ¥
                st.session_state.messages.append({"role": "assistant", "content": ai_response})
                st.chat_message("assistant").write(ai_response)
            except Exception as e:
                error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
                st.chat_message("assistant").write(error_msg)

# ë¬¸ì„œ í•™ìŠµ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°


    
